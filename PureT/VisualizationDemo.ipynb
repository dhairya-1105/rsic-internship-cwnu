{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pycocotools in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pycocotools) (3.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pycocotools) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocoevalcap in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pycocoevalcap) (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: fvcore in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.1.5.post20221221)\n",
      "Requirement already satisfied: numpy in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fvcore) (1.26.4)\n",
      "Requirement already satisfied: yacs>=0.1.6 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fvcore) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fvcore) (6.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fvcore) (4.66.5)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fvcore) (2.4.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fvcore) (10.3.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fvcore) (0.9.0)\n",
      "Requirement already satisfied: iopath>=0.1.7 in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fvcore) (0.1.10)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
      "Requirement already satisfied: portalocker in c:\\users\\dhair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from iopath>=0.1.7->fvcore) (3.1.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\dhair\\appdata\\roaming\\python\\python312\\site-packages (from portalocker->iopath>=0.1.7->fvcore) (306)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhair\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->fvcore) (0.4.6)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\dhair\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "import tqdm\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "\n",
    "import losses\n",
    "import models\n",
    "import datasets\n",
    "import lib.utils as utils\n",
    "from lib.utils import AverageMeter\n",
    "from optimizer.optimizer import Optimizer\n",
    "from evaluation.evaler import Evaler\n",
    "from scorer.scorer import Scorer\n",
    "from lib.config import cfg, cfg_from_file\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "# from timm.data.transforms import _pil_interp\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from fvcore.nn import FlopCountAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester(object):\n",
    "    def __init__(self, args):\n",
    "        super(Tester, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.vocab = utils.load_vocab(args.vocab)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((384, 384), interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)]\n",
    "        )\n",
    "\n",
    "        self.setup_network()\n",
    "\n",
    "    def setup_network(self):\n",
    "        model = models.create(cfg.MODEL.TYPE)\n",
    "        print(model)\n",
    "        self.model = torch.nn.DataParallel(model).cuda()\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Total Parameters: {total_params / 1e6:.2f}M\")   \n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable Parameters: {trainable_params / 1e6:.2f}M\")\n",
    "        # dummy_input = torch.randn(1, 3, 384, 384).cuda()\n",
    "        # flops = FlopCountAnalysis(model, dummy_input)\n",
    "        # print(f\"GFLOPs: {flops.total() / 1e9:.2f}\")  \n",
    "        if self.args.resume > 0:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.snapshot_path(\"caption_model\", self.args.resume),\n",
    "                    map_location=lambda storage, loc: storage)\n",
    "            )\n",
    "            \n",
    "    def make_kwargs(self, indices, ids, gv_feat, att_feats, att_mask):\n",
    "        kwargs = {}\n",
    "        kwargs[cfg.PARAM.INDICES] = indices\n",
    "        kwargs[cfg.PARAM.GLOBAL_FEAT] = gv_feat\n",
    "        kwargs[cfg.PARAM.ATT_FEATS] = att_feats\n",
    "        kwargs[cfg.PARAM.ATT_FEATS_MASK] = att_mask\n",
    "        kwargs['BEAM_SIZE'] = 5\n",
    "        kwargs['GREEDY_DECODE'] = True\n",
    "        return kwargs\n",
    "    \n",
    "    def read_img(self, image):\n",
    "        img = cv2.imread(image)\n",
    "        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        att_feats = self.transform(img)[None].cuda()\n",
    "        return att_feats\n",
    "    \n",
    "    def inference_img(self, image):\n",
    "        with torch.no_grad():\n",
    "            indices = 0\n",
    "            ids = image\n",
    "            gv_feat = None\n",
    "            att_feats = self.read_img(image)\n",
    "            att_mask = torch.ones(1, 12*12).cuda()\n",
    "            \n",
    "            kwargs = self.make_kwargs(indices, ids, gv_feat, att_feats, att_mask)\n",
    "            if kwargs['BEAM_SIZE'] > 1:\n",
    "                seq, _ = self.model.module.decode_beam(**kwargs)\n",
    "            else:\n",
    "                seq, _ = self.model.module.decode(**kwargs)\n",
    "                \n",
    "            sents = utils.decode_sequence(self.vocab, seq.data)\n",
    "            # print(ids, ''.join(sents[0].split(' ')))\n",
    "            return ' '.join(sents[0].split(' '))\n",
    "        \n",
    "    def eval(self, epoch, images):\n",
    "        self.model.eval()\n",
    "        \n",
    "        caps = []\n",
    "        for image in images:\n",
    "            cap = self.inference_img(image)\n",
    "            caps.append(cap)\n",
    "        return caps\n",
    "            \n",
    "\n",
    "    def snapshot_path(self, name, epoch):\n",
    "        snapshot_folder = os.path.join(cfg.ROOT_DIR, 'snapshot')\n",
    "        return os.path.join(snapshot_folder, name + \"_\" + str(epoch) + \".pth\")\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse input arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Image Captioning')\n",
    "    parser.add_argument('--folder', dest='folder', default=None, type=str)\n",
    "    parser.add_argument(\"--resume\", type=int, default=-1)\n",
    "    parser.add_argument(\"--vocab\", type=str, \n",
    "                        default=r'C:\\Users\\dhair\\Documents\\VS-Code-Practice-Files-main\\CWNU Intern Work\\PureT\\coco_vocabulary.txt')\n",
    "    # parser.add_argument(\"--images\", type=str, nargs='+', default='')\n",
    "\n",
    "    if len(sys.argv) == 1:\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "\n",
    "    args = parser.parse_args(args=['--folder', './experiments_PureT/PureT_SCST/', '--resume', '27'])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called with args:\n",
      "Namespace(folder='./experiments_PureT/PureT_SCST/', resume=27, vocab='C:\\\\Users\\\\dhair\\\\Documents\\\\VS-Code-Practice-Files-main\\\\CWNU Intern Work\\\\PureT\\\\coco_vocabulary.txt')\n",
      "load pretrained weights!\n",
      "PureT(\n",
      "  (backbone): SwinTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        dim=192, input_resolution=(96, 96), depth=2\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=192, input_resolution=(96, 96), num_heads=6, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=192, window_size=(12, 12), num_heads=6\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=192, input_resolution=(96, 96), num_heads=6, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=192, window_size=(12, 12), num_heads=6\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.004)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          input_resolution=(96, 96), dim=192\n",
      "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        dim=384, input_resolution=(48, 48), depth=2\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=384, input_resolution=(48, 48), num_heads=12, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=384, window_size=(12, 12), num_heads=12\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.009)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=384, input_resolution=(48, 48), num_heads=12, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=384, window_size=(12, 12), num_heads=12\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.013)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          input_resolution=(48, 48), dim=384\n",
      "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        dim=768, input_resolution=(24, 24), depth=18\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.017)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.022)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.026)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.030)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.035)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.039)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.043)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.048)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.052)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.057)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.061)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.065)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.070)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.074)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.078)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.083)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.087)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): SwinTransformerBlock(\n",
      "            dim=768, input_resolution=(24, 24), num_heads=24, window_size=12, shift_size=6, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=768, window_size=(12, 12), num_heads=24\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.091)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          input_resolution=(24, 24), dim=768\n",
      "          (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        dim=1536, input_resolution=(12, 12), depth=2\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            dim=1536, input_resolution=(12, 12), num_heads=48, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=1536, window_size=(12, 12), num_heads=48\n",
      "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.096)\n",
      "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            dim=1536, input_resolution=(12, 12), num_heads=48, window_size=12, shift_size=0, mlp_ratio=4.0\n",
      "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              dim=1536, window_size=(12, 12), num_heads=48\n",
      "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.100)\n",
      "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (att_embed): Sequential(\n",
      "    (0): Linear(in_features=1536, out_features=512, bias=True)\n",
      "    (1): CELU(alpha=1.3, inplace=True)\n",
      "    (2): Identity()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x EncoderLayer(\n",
      "        (encoder_attn): WindowAttention(\n",
      "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (o_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer): FeedForward(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): ReLU()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x DecoderLayer(\n",
      "        (word_attn): MultiHeadSelfAttention(\n",
      "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (o_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (cross_att): MultiHeadSelfAttention(\n",
      "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (o_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer): FeedForward(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): ReLU()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fuse_layer): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (fuse_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (word_embed): Embedding(9488, 512)\n",
      "    (pos_embed): Embedding(100, 512)\n",
      "    (generator): Linear(in_features=512, out_features=9488, bias=True)\n",
      "  )\n",
      ")\n",
      "Total Parameters: 229.41M\n",
      "Trainable Parameters: 34.16M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhair\\AppData\\Local\\Temp\\ipykernel_17248\\1354802261.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(self.snapshot_path(\"caption_model\", self.args.resume),\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "    \n",
    "print('Called with args:')\n",
    "print(args)\n",
    "\n",
    "if args.folder is not None:\n",
    "    cfg_from_file(os.path.join(args.folder, 'config.yml'))\n",
    "cfg.ROOT_DIR = args.folder\n",
    "\n",
    "tester = Tester(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def vis_img_cap(img_files, caps):\n",
    "    assert len(img_files) == len(caps), 'error'\n",
    "    for i in range(len(img_files)):\n",
    "        img_file = img_files[i]\n",
    "        cap = caps[i]\n",
    "        img = cv2.imread(img_file)\n",
    "        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        print(img_file, cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "# Paths\n",
    "# Paths (assuming RSICD dataset is in the current directory)\n",
    "ann_dir = r'C:\\Users\\dhair\\Documents\\VS-Code-Practice-Files-main\\CWNU Intern Work\\RSICD\\annotations'\n",
    "img_dir = r'C:\\Users\\dhair\\Documents\\VS-Code-Practice-Files-main\\CWNU Intern Work\\RSICD\\images'\n",
    "json_files = ['RSICD_test.json']\n",
    "\n",
    "# Check if directories exist\n",
    "if not os.path.exists(ann_dir):\n",
    "    raise FileNotFoundError(f\"Annotations directory not found at {ann_dir}\")\n",
    "if not os.path.exists(img_dir):\n",
    "    raise FileNotFoundError(f\"Images directory not found at {img_dir}\")\n",
    "\n",
    "# Step 1: Build image-to-captions mapping\n",
    "gt_dict = {}\n",
    "for file in json_files:\n",
    "    with open(os.path.join(ann_dir, file)) as f:\n",
    "        data = json.load(f)\n",
    "        for entry in data:\n",
    "            fname = entry['image']\n",
    "            caption = entry['caption']\n",
    "            gt_dict.setdefault(fname, []).append(caption)   \n",
    "\n",
    "# Step 2: Build image path list\n",
    "# img_files = [os.path.join(img_dir, fname) for fname in gt_dict]\n",
    "img_files = [os.path.join(img_dir, fname) for fname in list(gt_dict)[:]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1094"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Inference\n",
    "\n",
    "caps = tester.eval(args.resume, img_files)  # preds\n",
    "refs = [gt_dict[os.path.basename(p)] for p in img_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: a view of a city from an airport\n",
      "References:\n",
      "  1. ['there are a lot of buildings at the airport .', 'there are several square at the airport .', 'a plane is near a building and a runway in an airport .', 'a plane is near a building and a runway in an airport .', 'there are a lot of buildings at the airport .']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_image_and_captions(index, img_files, caps, gt_dict):\n",
    "    \"\"\"\n",
    "    Loads image, 5 reference captions, and predicted caption for a given index.\n",
    "    \n",
    "    Args:\n",
    "        index (int): Index of the image in img_files.\n",
    "        img_files (List[str]): List of image file paths.\n",
    "        caps (List[str]): List of predicted captions (aligned with img_files).\n",
    "        gt_dict (Dict[str, List[str]]): Mapping from image filename to reference captions.\n",
    "\n",
    "    Returns:\n",
    "        image (PIL.Image): The loaded image.\n",
    "        references (List[str]): List of 5 ground truth captions.\n",
    "        prediction (str): Predicted caption.\n",
    "    \"\"\"\n",
    "    img_path = img_files[index]\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    fname = os.path.basename(img_path)\n",
    "    references = gt_dict.get(fname, [])\n",
    "    prediction = caps[index]\n",
    "\n",
    "    return image, references, prediction\n",
    "\n",
    "\n",
    "# Example: View image and captions at index 7\n",
    "index = 7\n",
    "image, references, prediction = load_image_and_captions(index, img_files, caps, gt_dict)\n",
    "\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(\"References:\")\n",
    "for i, ref in enumerate(references):\n",
    "    print(f\"  {i+1}. {ref}\")\n",
    "\n",
    "# If using Jupyter or IPython\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_dict = {str(i): refs[i] for i in range(len(refs))}\n",
    "predictions_dict = {str(i): [caps[i]] for i in range(len(caps))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved refs_dict.json and preds_dict.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"preds_dict.json\", \"w\") as f:\n",
    "    json.dump(predictions_dict, f, indent=2)\n",
    "\n",
    "with open(\"refs_dict.json\", \"w\") as f:\n",
    "    json.dump(references_dict, f, indent=2)\n",
    "\n",
    "print(\"Saved refs_dict.json and preds_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [['the tarmac and airport runways divide the field into several orderly arranged rounded rectangles next to which is buildings and a road.',\n",
       "   'the tarmac and airport runways divide the field into several orderly arranged rounded rectangles next to which is buildings and a road.',\n",
       "   'a brown ground divided by the grey runway .',\n",
       "   'we can see a simple terminal building and an apron connected with runways',\n",
       "   'some building with a parking lot are near an airport with several runways .']],\n",
       " '1': [['many white planes are parked at the airport .',\n",
       "   'a highway is built next to the airport .',\n",
       "   'a highway is built next to the airport .',\n",
       "   'many white planes are parked at the airport .',\n",
       "   'many white planes are parked at the airport .']],\n",
       " '2': [['a parking apron with a plane parked on and connected to a runway is lying on the bare land near which there are some square buildings.',\n",
       "   'a parking apron with a plane parked on and connected to a runway is lying on the bare land near which there are some square buildings.',\n",
       "   'the airport is between a runway and the farming land .',\n",
       "   'there is a simple terminal building with two terminal besides the apron which is connected to the runway',\n",
       "   'a plane is near some buildings and a runway in an airport .']],\n",
       " '3': [['a polygonal terminal building is built between a square surrounded by roads and runways built in the field',\n",
       "   'a polygonal terminal building is built between a square surrounded by roads and runways built in the field',\n",
       "   'a square boarding gate next to the runway and some blocks .',\n",
       "   'a squared terminal building sits beside an apron which is connected to the runway',\n",
       "   'three planes are near a white building in an airport with several runways .']],\n",
       " '4': [['the polygonal terminal building is built near road and other buildings on the other side of which is tarmac and runways',\n",
       "   'the polygonal terminal building is built near road and other buildings on the other side of which is tarmac and runways',\n",
       "   'the alternation runway next to a bright blue parking lot',\n",
       "   'a dazzling apron is surrounded by the terminal building and six block of houses and two runway',\n",
       "   'a plane is on the runway while other two planes are near some buildings and green plants .']],\n",
       " '5': [['the terminal building built on the parking apron is semi closed with some plane around.',\n",
       "   'the terminal building built on the parking apron is semi closed with some plane around.',\n",
       "   'a white door shaped boarding gate and a polygon runway .',\n",
       "   'there is a large apron with an irregular shaped terminal building on it',\n",
       "   'several white planes are parked near a large building with a parking lot in an airport .']],\n",
       " '6': [['many planes are parked in an airport .',\n",
       "   'many planes are parked in an airport .',\n",
       "   'many planes are parked in an airport .',\n",
       "   'many planes are parked in an airport .',\n",
       "   'many planes are parked in an airport .']],\n",
       " '7': [['there are a lot of buildings at the airport .',\n",
       "   'there are several square at the airport .',\n",
       "   'a plane is near a building and a runway in an airport .',\n",
       "   'a plane is near a building and a runway in an airport .',\n",
       "   'there are a lot of buildings at the airport .']],\n",
       " '8': [['a airport with many buildings beside in it .',\n",
       "   'the airport is surrounded by green land and brown plants .',\n",
       "   'a plane is in an airport .',\n",
       "   'a plane is in an airport .',\n",
       "   'a airport with many buildings beside in it .']],\n",
       " '9': [['the ground has a large and small aircraft .',\n",
       "   'the airport facilities are complete .',\n",
       "   'five planes are near a large building in an airport .',\n",
       "   'five planes are near a large building in an airport .',\n",
       "   'the ground has a large and small aircraft .']]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring average inference time...\n",
      "Avg inference time: 416.52 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_runs = 100\n",
    "total_time = 0\n",
    "\n",
    "print(\"Measuring average inference time...\")\n",
    "for i in range(num_runs):\n",
    "    img_path = random.choice(img_files)\n",
    "    start = time.time()\n",
    "    _ = tester.inference_img(img_path)\n",
    "    total_time += (time.time() - start)\n",
    "\n",
    "avg_time = total_time / num_runs\n",
    "print(f\"Avg inference time: {avg_time * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFlops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::mul encountered 73 time(s)\n",
      "Unsupported operator aten::add encountered 83 time(s)\n",
      "Unsupported operator aten::softmax encountered 24 time(s)\n",
      "Unsupported operator aten::gelu encountered 24 time(s)\n",
      "Unsupported operator aten::bernoulli_ encountered 46 time(s)\n",
      "Unsupported operator aten::div_ encountered 46 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone FLOPs: 104.08 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::mul encountered 4 time(s)\n",
      "Unsupported operator aten::sum encountered 5 time(s)\n",
      "Unsupported operator aten::div encountered 1 time(s)\n",
      "Unsupported operator aten::repeat encountered 9 time(s)\n",
      "Unsupported operator aten::add encountered 10 time(s)\n",
      "Unsupported operator aten::mean encountered 6 time(s)\n",
      "Unsupported operator aten::softmax encountered 6 time(s)\n",
      "Unsupported operator aten::pad encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder FLOPs:  1.40 GFLOPs\n",
      "Total (backbone + encoder): 105.48 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "puret_model = tester.model.module\n",
    "\n",
    "# Calculate backbone FLOPs\n",
    "dummy = torch.randn(1, 3, 384, 384).cuda()\n",
    "backbone_flops = FlopCountAnalysis(puret_model.backbone, dummy)\n",
    "print(f\"Backbone FLOPs: {backbone_flops.total() / 1e9:.2f} GFLOPs\")\n",
    "\n",
    "# Get features from backbone\n",
    "att_feats = puret_model.backbone(dummy)\n",
    "# print(f\"att_feats shape: {att_feats.shape}\")  # Debug shape\n",
    "\n",
    "# Process features through att_embed\n",
    "att_feats = puret_model.att_embed(att_feats)\n",
    "# print(f\"embedded att_feats shape: {att_feats.shape}\")  # Debug shape\n",
    "\n",
    "# Calculate encoder FLOPs\n",
    "att_mask = torch.ones(1, att_feats.shape[1]).cuda()\n",
    "encoder_flops = FlopCountAnalysis(puret_model.encoder, (att_feats, att_mask))\n",
    "print(f\"Encoder FLOPs:  {encoder_flops.total() / 1e9:.2f} GFLOPs\")\n",
    "\n",
    "total = backbone_flops.total() + encoder_flops.total()\n",
    "print(f\"Total (backbone + encoder): {total / 1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 229.41M\n",
      "Trainable Parameters: 34.16M\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in puret_model.parameters())\n",
    "print(f\"Total Parameters: {total_params / 1e6:.2f}M\")   \n",
    "trainable_params = sum(p.numel() for p in puret_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params / 1e6:.2f}M\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
